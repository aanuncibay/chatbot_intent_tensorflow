{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# things we need for NLP\r\n",
    "\r\n",
    "import nltk\r\n",
    "import tensorflow as tf\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import pickle\r\n",
    "import json\r\n",
    "\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\r\n",
    "from tensorflow.keras.optimizers import SGD\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "lemmatizer = WordNetLemmatizer()\r\n",
    "intents = json.loads(open('./data/intents.json', encoding = \"utf-8\").read())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(intents)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'intents': [{'tag': 'bienvenida', 'patterns': ['Hola', 'hay alguien hay', 'buenos días ', 'buen día', 'Hooooooola', 'qué tal'], 'responses': ['Hola', 'Buenos días', 'Buenos días \\n ¡Qué gusto tenerte por acá!', 'Hola ¿en qué te puedo ayudar?']}, {'tag': 'confirmacion', 'patterns': ['Gracias', 'vale', 'entiendo'], 'responses': ['Por nada, ¿algo más en que puedo ayudarte?', 'A tu disposición, ¿alguna otra cosa?', 'Un placer. ¿qué más puedo hacer por ti?']}, {'tag': 'medicamentos', 'patterns': ['medicina', 'mis medicamentos, cuales son', 'cuál medicina me toca ahora', 'pastillas', 'jarabe', 'no sé cuándo reocger mi medicamento', 'vacuna'], 'responses': ['Ya reviso tu historial, un segundo', 'Ya vamos a revisar qué pasa. Un minuto, por favor']}, {'tag': 'estado_de_animo', 'patterns': ['hoy no me he sentido bien', 'me siento mejor', 'a ver qué tal me siento', 'un poco de malestar'], 'responses': ['exactamente, ¿qué tienes?', '¿qué te ha pasado?', '¿quieres conversar sobre esto?']}, {'tag': 'cita', 'patterns': ['¿cuándo es mi próxima cita?', 'me puedes recordar mi próxima cita', 'no recuerdo mi cita', 'cuando me toca médico'], 'responses': ['Ya vamos a revisar. Un segundo']}, {'tag': 'despedida', 'patterns': ['Hasta otra ocasión', 'Hasta luego', 'Adios', 'chao', 'me desconecto', 'bye', 'nos vemos luego'], 'responses': ['Hasta luego', '¡Qué tengas un buen día!', 'Hasta luego, nos vemos pronto']}]}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "words = []\r\n",
    "classes = []\r\n",
    "documents = []\r\n",
    "ignore_words = ['?', ',', '!']\r\n",
    "\r\n",
    "for intent in intents['intents']: \r\n",
    "    for pattern in intent['patterns']:\r\n",
    "        word_list = nltk.word_tokenize(pattern)\r\n",
    "        words.extend(word_list)\r\n",
    "        documents.append((word_list, intent['tag']))\r\n",
    "        if intent['tag'] not in classes: \r\n",
    "            classes.append(intent['tag'])\r\n",
    "print(documents)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(['Hola'], 'bienvenida'), (['hay', 'alguien', 'hay'], 'bienvenida'), (['buenos', 'días'], 'bienvenida'), (['buen', 'día'], 'bienvenida'), (['Hooooooola'], 'bienvenida'), (['qué', 'tal'], 'bienvenida'), (['Gracias'], 'confirmacion'), (['vale'], 'confirmacion'), (['entiendo'], 'confirmacion'), (['medicina'], 'medicamentos'), (['mis', 'medicamentos', ',', 'cuales', 'son'], 'medicamentos'), (['cuál', 'medicina', 'me', 'toca', 'ahora'], 'medicamentos'), (['pastillas'], 'medicamentos'), (['jarabe'], 'medicamentos'), (['no', 'sé', 'cuándo', 'reocger', 'mi', 'medicamento'], 'medicamentos'), (['vacuna'], 'medicamentos'), (['hoy', 'no', 'me', 'he', 'sentido', 'bien'], 'estado_de_animo'), (['me', 'siento', 'mejor'], 'estado_de_animo'), (['a', 'ver', 'qué', 'tal', 'me', 'siento'], 'estado_de_animo'), (['un', 'poco', 'de', 'malestar'], 'estado_de_animo'), (['¿cuándo', 'es', 'mi', 'próxima', 'cita', '?'], 'cita'), (['me', 'puedes', 'recordar', 'mi', 'próxima', 'cita'], 'cita'), (['no', 'recuerdo', 'mi', 'cita'], 'cita'), (['cuando', 'me', 'toca', 'médico'], 'cita'), (['Hasta', 'otra', 'ocasión'], 'despedida'), (['Hasta', 'luego'], 'despedida'), (['Adios'], 'despedida'), (['chao'], 'despedida'), (['me', 'desconecto'], 'despedida'), (['bye'], 'despedida'), (['nos', 'vemos', 'luego'], 'despedida')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\r\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in ignore_words]\r\n",
    "words = sorted(list(set(words)))\r\n",
    "classes = sorted(list(set(classes)))\r\n",
    "print(len(documents), \"documents\")\r\n",
    "print(len(classes), \"labels \\n\", classes)\r\n",
    "print(len(words), \"unique stemmed words \\n\", words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "31 documents\n",
      "6 labels \n",
      " ['bienvenida', 'cita', 'confirmacion', 'despedida', 'estado_de_animo', 'medicamentos']\n",
      "60 unique stemmed words \n",
      " ['a', 'adios', 'ahora', 'alguien', 'bien', 'buen', 'buenos', 'bye', 'chao', 'cita', 'cuales', 'cuando', 'cuál', 'cuándo', 'de', 'desconecto', 'día', 'días', 'e', 'entiendo', 'gracias', 'hasta', 'hay', 'he', 'hola', 'hooooooola', 'hoy', 'jarabe', 'luego', 'malestar', 'me', 'medicamento', 'medicamentos', 'medicina', 'mejor', 'mi', 'médico', 'no', 'ocasión', 'otra', 'pastillas', 'poco', 'próxima', 'puedes', 'qué', 'recordar', 'recuerdo', 'reocger', 'sentido', 'siento', 'son', 'sé', 'tal', 'toca', 'un', 'vacuna', 'vale', 'vemos', 'ver', '¿cuándo']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "pickle.dump(words, open('./data/words.pkl', 'wb'))\r\n",
    "pickle.dump(classes, open('./data/classes.pkl', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#vectorización de los patrones para la DNN\r\n",
    "\r\n",
    "training = []\r\n",
    "output = []\r\n",
    "output_empty = [0] * len(classes)\r\n",
    "\r\n",
    "for doc in documents: \r\n",
    "    bag = []\r\n",
    "    pattern_words = doc[0]\r\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words] \r\n",
    "    for w in words:\r\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\r\n",
    "    output_row = list(output_empty)\r\n",
    "    output_row[classes.index(doc[1])] = 1\r\n",
    "    training.append([bag, output_row])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# shuffle our features and turn into np.array\r\n",
    "random.shuffle(training)\r\n",
    "training = np.array(training, dtype=\"object\")\r\n",
    "\r\n",
    "# create train and test lists\r\n",
    "train_x = list(training[:,0])\r\n",
    "train_y = list(training[:,1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model = Sequential()\r\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation = 'relu'))\r\n",
    "model.add(Dropout(0.5))\r\n",
    "model.add(Dense(64, activation = 'relu'))\r\n",
    "model.add(Dropout(0.5))\r\n",
    "model.add(Dense(len(train_y[0]), activation = 'softmax'))\r\n",
    "\r\n",
    "sgd = SGD(learning_rate= 0.01, decay = 1e-6, momentum=0.9, nesterov = True)\r\n",
    "\r\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics =['accuracy'])\r\n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs = 200, batch_size = 5, verbose = 0)\r\n",
    "\r\n",
    "print(\"Listo, Calisto\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Listo, Calisto\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model.save('./data/chatbot_model.h5', hist)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "interpreter": {
   "hash": "2d8c4e0adecca2f9bb6152916de834ccf75a0d1de20f0239611dabd3480e2716"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}